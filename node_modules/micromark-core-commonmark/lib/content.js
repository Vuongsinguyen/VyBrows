/**
<<<<<<< HEAD
 * @typedef {import('micromark-util-types').Construct} Construct
 * @typedef {import('micromark-util-types').Resolver} Resolver
 * @typedef {import('micromark-util-types').State} State
 * @typedef {import('micromark-util-types').Token} Token
 * @typedef {import('micromark-util-types').TokenizeContext} TokenizeContext
 * @typedef {import('micromark-util-types').Tokenizer} Tokenizer
 */

import {factorySpace} from 'micromark-factory-space'
import {markdownLineEnding} from 'micromark-util-character'
import {subtokenize} from 'micromark-util-subtokenize'
=======
 * @import {
 *   Construct,
 *   Resolver,
 *   State,
 *   TokenizeContext,
 *   Tokenizer,
 *   Token
 * } from 'micromark-util-types'
 */

import { factorySpace } from 'micromark-factory-space';
import { markdownLineEnding } from 'micromark-util-character';
import { subtokenize } from 'micromark-util-subtokenize';
>>>>>>> 08f40ceb (Initial)
/**
 * No name because it must not be turned off.
 * @type {Construct}
 */
export const content = {
<<<<<<< HEAD
  tokenize: tokenizeContent,
  resolve: resolveContent
}

/** @type {Construct} */
const continuationConstruct = {
  tokenize: tokenizeContinuation,
  partial: true
}
=======
  resolve: resolveContent,
  tokenize: tokenizeContent
};

/** @type {Construct} */
const continuationConstruct = {
  partial: true,
  tokenize: tokenizeContinuation
};
>>>>>>> 08f40ceb (Initial)

/**
 * Content is transparent: itâ€™s parsed right now. That way, definitions are also
 * parsed right now: before text in paragraphs (specifically, media) are parsed.
 *
 * @type {Resolver}
 */
function resolveContent(events) {
<<<<<<< HEAD
  subtokenize(events)
  return events
=======
  subtokenize(events);
  return events;
>>>>>>> 08f40ceb (Initial)
}

/**
 * @this {TokenizeContext}
<<<<<<< HEAD
=======
 *   Context.
>>>>>>> 08f40ceb (Initial)
 * @type {Tokenizer}
 */
function tokenizeContent(effects, ok) {
  /** @type {Token | undefined} */
<<<<<<< HEAD
  let previous
  return chunkStart
=======
  let previous;
  return chunkStart;
>>>>>>> 08f40ceb (Initial)

  /**
   * Before a content chunk.
   *
   * ```markdown
   * > | abc
   *     ^
   * ```
   *
   * @type {State}
   */
  function chunkStart(code) {
<<<<<<< HEAD
    effects.enter('content')
    previous = effects.enter('chunkContent', {
      contentType: 'content'
    })
    return chunkInside(code)
=======
    effects.enter("content");
    previous = effects.enter("chunkContent", {
      contentType: "content"
    });
    return chunkInside(code);
>>>>>>> 08f40ceb (Initial)
  }

  /**
   * In a content chunk.
   *
   * ```markdown
   * > | abc
   *     ^^^
   * ```
   *
   * @type {State}
   */
  function chunkInside(code) {
    if (code === null) {
<<<<<<< HEAD
      return contentEnd(code)
=======
      return contentEnd(code);
>>>>>>> 08f40ceb (Initial)
    }

    // To do: in `markdown-rs`, each line is parsed on its own, and everything
    // is stitched together resolving.
    if (markdownLineEnding(code)) {
<<<<<<< HEAD
      return effects.check(
        continuationConstruct,
        contentContinue,
        contentEnd
      )(code)
    }

    // Data.
    effects.consume(code)
    return chunkInside
=======
      return effects.check(continuationConstruct, contentContinue, contentEnd)(code);
    }

    // Data.
    effects.consume(code);
    return chunkInside;
>>>>>>> 08f40ceb (Initial)
  }

  /**
   *
   *
   * @type {State}
   */
  function contentEnd(code) {
<<<<<<< HEAD
    effects.exit('chunkContent')
    effects.exit('content')
    return ok(code)
=======
    effects.exit("chunkContent");
    effects.exit("content");
    return ok(code);
>>>>>>> 08f40ceb (Initial)
  }

  /**
   *
   *
   * @type {State}
   */
  function contentContinue(code) {
<<<<<<< HEAD
    effects.consume(code)
    effects.exit('chunkContent')
    previous.next = effects.enter('chunkContent', {
      contentType: 'content',
      previous
    })
    previous = previous.next
    return chunkInside
=======
    effects.consume(code);
    effects.exit("chunkContent");
    previous.next = effects.enter("chunkContent", {
      contentType: "content",
      previous
    });
    previous = previous.next;
    return chunkInside;
>>>>>>> 08f40ceb (Initial)
  }
}

/**
 * @this {TokenizeContext}
<<<<<<< HEAD
 * @type {Tokenizer}
 */
function tokenizeContinuation(effects, ok, nok) {
  const self = this
  return startLookahead
=======
 *   Context.
 * @type {Tokenizer}
 */
function tokenizeContinuation(effects, ok, nok) {
  const self = this;
  return startLookahead;
>>>>>>> 08f40ceb (Initial)

  /**
   *
   *
   * @type {State}
   */
  function startLookahead(code) {
<<<<<<< HEAD
    effects.exit('chunkContent')
    effects.enter('lineEnding')
    effects.consume(code)
    effects.exit('lineEnding')
    return factorySpace(effects, prefixed, 'linePrefix')
=======
    effects.exit("chunkContent");
    effects.enter("lineEnding");
    effects.consume(code);
    effects.exit("lineEnding");
    return factorySpace(effects, prefixed, "linePrefix");
>>>>>>> 08f40ceb (Initial)
  }

  /**
   *
   *
   * @type {State}
   */
  function prefixed(code) {
    if (code === null || markdownLineEnding(code)) {
<<<<<<< HEAD
      return nok(code)
=======
      return nok(code);
>>>>>>> 08f40ceb (Initial)
    }

    // Always populated by defaults.

<<<<<<< HEAD
    const tail = self.events[self.events.length - 1]
    if (
      !self.parser.constructs.disable.null.includes('codeIndented') &&
      tail &&
      tail[1].type === 'linePrefix' &&
      tail[2].sliceSerialize(tail[1], true).length >= 4
    ) {
      return ok(code)
    }
    return effects.interrupt(self.parser.constructs.flow, nok, ok)(code)
  }
}
=======
    const tail = self.events[self.events.length - 1];
    if (!self.parser.constructs.disable.null.includes('codeIndented') && tail && tail[1].type === "linePrefix" && tail[2].sliceSerialize(tail[1], true).length >= 4) {
      return ok(code);
    }
    return effects.interrupt(self.parser.constructs.flow, nok, ok)(code);
  }
}
>>>>>>> 08f40ceb (Initial)
